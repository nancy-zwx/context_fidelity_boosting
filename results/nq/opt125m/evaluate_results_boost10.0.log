/mnt/nlp/gaoqiang/miniconda3/envs/DeepSeekMoE/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO:__main__:Loading tokenizer...
INFO:__main__:Loading FactKB model...
/mnt/nlp/gaoqiang/miniconda3/envs/DeepSeekMoE/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
ERROR:__main__:Error loading models: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 15.94 MiB is free. Process 3087631 has 74.04 GiB memory in use. Process 3852442 has 1.16 GiB memory in use. Process 3886321 has 1.05 GiB memory in use. Process 3892746 has 2.16 GiB memory in use. Including non-PyTorch memory, this process has 684.00 MiB memory in use. Of the allocated memory 258.17 MiB is allocated by PyTorch, and 11.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:__main__:Failed to load models. Exiting...
