{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Fidelity Boosting Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eb174c35c94180adf0c6631295c838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList, LogitsProcessor\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = test_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token = test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\"\n",
    "\n",
    "context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "\n",
    "\n",
    "def standard_decoding(input_ids, max_length=128, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def context_aware_sampling(model, tokenizer, input_ids, context_ids, alpha=0.9, max_length=128, temperature=1.0):\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            full_context_outputs = model(generated_tokens)\n",
    "            full_context_logits = full_context_outputs.logits[:, -1, :] \n",
    "\n",
    "            question_only_input = generated_tokens[:, len(context_ids):]\n",
    "            question_only_outputs = model(question_only_input)\n",
    "            question_only_logits = question_only_outputs.logits[:, -1, :] \n",
    "\n",
    "        adjusted_logits = (1 + alpha) * full_context_logits - alpha * question_only_logits\n",
    "        adjusted_probs = F.softmax(adjusted_logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "\n",
    "        generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Answer: \n",
      "Argentina has won **4** world cups. \n",
      "\n",
      "\n",
      "**Note:** You will need to confirm the date if this is accurate and what happened in 2026. \n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Context-Aware Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Here's the solution:\n",
      "\n",
      "* ** Argentina won the World Cup in 1978, 1986, 2022, and 2026 in the year shown.** \n",
      "*  Argentina has won **four** World Cups before 2027 \n",
      "\n",
      "Let me know if you have any other tricky logic puzzles for me to solve! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "standard_output = standard_decoding(input_ids)\n",
    "output_tokens = context_aware_sampling(\n",
    "                                        model,\n",
    "                                        tokenizer,\n",
    "                                        input_ids,\n",
    "                                        context_ids=context_input,\n",
    "                                        alpha=0.5,\n",
    "                                        max_length=128,\n",
    "                                        temperature=1.0,\n",
    "                                    )\n",
    "\n",
    "context_aware_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Standard Decoding Output:\\n\", standard_output)\n",
    "print(\"__\" * 50)\n",
    "print(\"Context-Aware Decoding Output:\\n\", context_aware_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watermark-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enhanced_decoding(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,  # 固定boost值\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 拼接输入\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    # 记录context长度,用于识别context tokens\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 2. 开始生成\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "    \n",
    "            # 创建boost mask - 对应context位置的tokens增加delta\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            \n",
    "            # 获取当前位置之前的tokens\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            # 找到context tokens在词表中的index\n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = delta\n",
    "                \n",
    "            # 应用boost\n",
    "            adjusted_logits = logits + boost_mask\n",
    "            \n",
    "            # 应用temperature\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            \n",
    "            # 转换为概率\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            \n",
    "            # 采样下一个token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 拼接到生成序列\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            # 检查是否生成结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "def standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 标准生成方式作为对比\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Answer: 3\n",
      "\n",
      "Context-enhanced decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "The answer is 3. \n",
      " \n",
      " Argentina won in 1978, 1986, and 2022. \n",
      "\n",
      "The year 2026 is in the future and Argentina won in 2022. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "result = context_enhanced_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "standard_result = standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"\\nStandard decoding:\", standard_result)\n",
    "print(\"Context-enhanced decoding:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以进一步优化的方向：\n",
    "1. 动态delta:\n",
    "根据token重要性计算delta\n",
    "delta = compute_importance(token)\n",
    "2. 语义聚类\n",
    "对语义相近的tokens也增加权重\n",
    "similar_tokens = get_semantic_cluster(token)\n",
    "for t in similar_tokens:\n",
    "    boost_mask[0, t] = delta * similarity_score\n",
    "3. 更复杂的重要性分数计算：\n",
    "结合attention分数、熵值等计算重要性\n",
    "importance = compute_token_importance(\n",
    "    token,\n",
    "    attention_scores,\n",
    "    semantic_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自适应delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: How many world cups has Argentina won?\n",
      "\n",
      "Argentina has won the **World Cup three times**:\n",
      "\n",
      "* **1978**\n",
      "* **1986**\n",
      "* **2022** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_result = model.generate(\n",
    "    tokenizer(question, return_tensors=\"pt\").to(device).input_ids,\n",
    "    max_length=128,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "standard_text = tokenizer.decode(standard_result[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nStandard decoding:\", standard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局自适应增强\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jsd(p_logits, q_logits):\n",
    "    p = F.softmax(p_logits, dim=-1)\n",
    "    q = F.softmax(q_logits, dim=-1)\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    jsd = 0.5 * (F.kl_div(m.log(), p, reduction='batchmean') + \n",
    "                 F.kl_div(m.log(), q, reduction='batchmean'))\n",
    "    return jsd\n",
    "\n",
    "def calculate_distribution_difference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question\n",
    "):\n",
    "    \"\"\"计算有无上下文时的预测分布差异\"\"\"\n",
    "    \n",
    "    with_context_input = tokenizer(context + question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with_context_outputs = model(with_context_input)\n",
    "    with_context_logits = with_context_outputs.logits[:, -1, :]\n",
    "    \n",
    "    no_context_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    no_context_outputs = model(no_context_input)\n",
    "    no_context_logits = no_context_outputs.logits[:, -1, :]\n",
    "    \n",
    "    # 计算JSD\n",
    "    dist_diff = compute_jsd(with_context_logits, no_context_logits)\n",
    "    \n",
    "    return dist_diff.item()\n",
    "\n",
    "def adaptive_delta(distribution_diff, base_delta=2.0, scale=1.0):\n",
    "    \"\"\"根据分布差异动态调整boost强度\"\"\"\n",
    "    return base_delta * (1 + scale * distribution_diff)\n",
    "\n",
    "def global_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    scale=1.0,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 计算分布差异\n",
    "    dist_diff = calculate_distribution_difference(model, tokenizer, context, question)\n",
    "    \n",
    "    # 2. 计算自适应delta\n",
    "    current_delta = adaptive_delta(dist_diff, base_delta, scale)\n",
    "    \n",
    "    print(f\"Distribution difference: {dist_diff:.4f}\")\n",
    "    print(f\"Adaptive delta: {current_delta:.4f}\")\n",
    "    \n",
    "    # 3. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 4. 生成过程\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = current_delta\n",
    "                \n",
    "            adjusted_logits = logits + boost_mask\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution difference: 0.1260\n",
      "Adaptive delta: 2.2520\n",
      "\n",
      "Global adaptive decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won? \n",
      " \n",
      " \n",
      "The information is from 2025, and Argentina won the World Cup in 2022 and 2026. \n",
      "\n",
      "The correct answer is 2. \n",
      "\n",
      "\n",
      "The question is a bit of a trick, it tries to mislead you by listing future World Cups. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_result = global_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    scale=1.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"\\nGlobal adaptive decoding:\", global_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态自适应增强\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语义相似度（全局+语义相似度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(model, token_id, context_embeddings):\n",
    "    \"\"\"语义相似度计算\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 获取token embedding\n",
    "        token_embedding = model.get_input_embeddings()(\n",
    "            torch.tensor([token_id]).to(context_embeddings.device)\n",
    "        )\n",
    "        \n",
    "        # 计算余弦相似度\n",
    "        similarities = F.cosine_similarity(\n",
    "            token_embedding,\n",
    "            context_embeddings,\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        return similarities.mean().item()\n",
    "\n",
    "def token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 计算全局分布差异\n",
    "    dist_diff = calculate_distribution_difference(model, tokenizer, context, question)\n",
    "    base_boost = adaptive_delta(dist_diff, base_delta)\n",
    "    \n",
    "    # 2. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 3. 预计算context embeddings\n",
    "    with torch.no_grad():\n",
    "        context_embeddings = model.get_input_embeddings()(context_input[0, :context_length])\n",
    "    \n",
    "    # 4. 生成过程\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 计算token-wise boost\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            for token in prefix_tokens:\n",
    "                # 计算语义相似度\n",
    "                semantic_sim = calculate_semantic_similarity(\n",
    "                    model, \n",
    "                    token,\n",
    "                    context_embeddings\n",
    "                )\n",
    "                semantic_score = (semantic_sim + 1) / 2\n",
    "                # 应用boost\n",
    "                boost_mask[0, token] = base_boost * semantic_score\n",
    "            \n",
    "            adjusted_logits = logits + boost_mask\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-wise adaptive decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "* Argentina won in 1978.\n",
      "* Argentina won in 1986. \n",
      "* Argentina won in 2022. \n",
      "* Argentina won in 2026. \n",
      "\n",
      "\n",
      "The question is asking for the total number of World Cups won, and the answer is 4. \n",
      "\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_wise_result = token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Token-wise adaptive decoding:\", token_wise_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意力分数（ 全局+注意力分数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_scores(model, input_ids, context_length):\n",
    "    \"\"\"获取注意力分数的通用实现\"\"\"\n",
    "    try:\n",
    "        # 直接使用output_attentions参数\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "        if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n",
    "            last_layer_attention = outputs.attentions[-1]  # [batch, heads, seq_len, seq_len]\n",
    "            averaged_attention = last_layer_attention.mean(dim=1)  # [batch, seq_len, seq_len]\n",
    "            scores = averaged_attention[0, -1, :context_length]  # [context_length]\n",
    "            return F.softmax(scores, dim=-1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to get attention scores: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # 使用hidden states计算注意力分数\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # [batch, seq_len, hidden_dim]\n",
    "        query = hidden_states[:, -1:]  # [batch, 1, hidden_dim]\n",
    "        key = hidden_states[:, :context_length]  # [batch, context_length, hidden_dim]\n",
    "        attention = torch.matmul(query, key.transpose(-2, -1))  # [batch, 1, context_length]\n",
    "        attention = attention / math.sqrt(query.size(-1))\n",
    "        scores = F.softmax(attention, dim=-1)[0, 0]  # [context_length]\n",
    "        return scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to compute attention using hidden states: {e}\")\n",
    "    \n",
    "    # Fallback: 使用均匀分布\n",
    "    return torch.ones(context_length).to(input_ids.device) / context_length\n",
    "\n",
    "def token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 计算全局分布差异\n",
    "    dist_diff = calculate_distribution_difference(model, tokenizer, context, question)\n",
    "    base_boost = adaptive_delta(dist_diff, base_delta)\n",
    "    \n",
    "    # 2. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 3. 生成过程\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取注意力分数\n",
    "            attention_scores = get_attention_scores(\n",
    "                model, \n",
    "                generated_tokens,\n",
    "                context_length\n",
    "            )  # [context_length]\n",
    "            \n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 计算token-wise boost\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            for idx, token in enumerate(prefix_tokens):\n",
    "                # 使用注意力分数作为重要性\n",
    "                importance = attention_scores[idx].item()\n",
    "                \n",
    "                # 应用boost\n",
    "                boost_mask[0, token] = base_boost * importance\n",
    "            \n",
    "            # 调整logits并采样\n",
    "            adjusted_logits = logits + boost_mask\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-wise adaptive decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "The answer is 3\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Argentina won the World Cup in **1978, 1986, and 2022.** \n",
      "* The information you provided is that they won in 1978,1986,2022 and 2026\n",
      "* Therefore, they have won 3 World Cups. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_wise_result = token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Token-wise adaptive decoding:\", token_wise_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意力分数（全局+语义相似度+注意力分数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(model, token_id, context_embeddings):\n",
    "    \"\"\"语义相似度计算\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 获取token embedding\n",
    "        token_embedding = model.get_input_embeddings()(\n",
    "            torch.tensor([token_id]).to(context_embeddings.device)\n",
    "        )   \n",
    "        # 计算余弦相似度\n",
    "        similarities = F.cosine_similarity(\n",
    "            token_embedding,\n",
    "            context_embeddings,\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        return similarities.mean().item()\n",
    "    \n",
    "def get_attention_scores(model, input_ids, context_length):\n",
    "    \"\"\"获取注意力分数\"\"\"\n",
    "    try:\n",
    "        # 使用output_attentions参数\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "        if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n",
    "            last_layer_attention = outputs.attentions[-1]  # [batch, heads, seq_len, seq_len]\n",
    "            averaged_attention = last_layer_attention.mean(dim=1)  # [batch, seq_len, seq_len]\n",
    "            scores = averaged_attention[0, -1, :context_length]  # [context_length]\n",
    "            return F.softmax(scores, dim=-1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to get attention scores: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # 使用hidden states计算注意力分数\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # [batch, seq_len, hidden_dim]\n",
    "        query = hidden_states[:, -1:]  # [batch, 1, hidden_dim]\n",
    "        key = hidden_states[:, :context_length]  # [batch, context_length, hidden_dim]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention = torch.matmul(query, key.transpose(-2, -1))  # [batch, 1, context_length]\n",
    "        attention = attention / math.sqrt(query.size(-1))\n",
    "        scores = F.softmax(attention, dim=-1)[0, 0]  # [context_length]\n",
    "        return scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to compute attention using hidden states: {e}\")\n",
    "    \n",
    "    # Fallback: 使用均匀分布\n",
    "    return torch.ones(context_length).to(input_ids.device) / context_length\n",
    "\n",
    "def token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    lambda1=0.6,  # 注意力权重\n",
    "    lambda2=0.4,  # 语义相似度权重\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 计算全局分布差异\n",
    "    dist_diff = calculate_distribution_difference(model, tokenizer, context, question)\n",
    "    base_boost = adaptive_delta(dist_diff, base_delta)\n",
    "    \n",
    "    # 2. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 3. 预计算context embeddings\n",
    "    with torch.no_grad():\n",
    "        context_embeddings = model.get_input_embeddings()(context_input[0, :context_length])\n",
    "    \n",
    "    # 4. 生成过程\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取注意力分数\n",
    "            attention_scores = get_attention_scores(\n",
    "                model, \n",
    "                generated_tokens,\n",
    "                context_length\n",
    "            )  # [context_length]\n",
    "            \n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 计算token-wise boost\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            for idx, token in enumerate(prefix_tokens):\n",
    "                # 获取语义相似度\n",
    "                semantic_sim = calculate_semantic_similarity(\n",
    "                    model, \n",
    "                    token,\n",
    "                    context_embeddings\n",
    "                )\n",
    "                semantic_score = (semantic_sim + 1) / 2\n",
    "                \n",
    "                # 确保注意力分数是标量\n",
    "                attn_score = attention_scores[idx].item()\n",
    "                \n",
    "                # 组合注意力分数和语义相似度\n",
    "                importance = lambda1 * attn_score + lambda2 * semantic_score\n",
    "                \n",
    "                # 应用boost\n",
    "                boost_mask[0, token] = base_boost * importance\n",
    "            \n",
    "            adjusted_logits = logits + boost_mask\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced result: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "The answer is 3. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* Argentina won in 1978, 1986, and 2022. \n",
      "* The provided information is already clear. \n",
      " \n",
      "The question asks for the number of World Cups won, and the answer is 3 based on the provided information. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = token_wise_adaptive_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    lambda1=0.6,\n",
    "    lambda2=0.4,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Enhanced result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态权重调整 （abandoned）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_weights(attention_scores, semantic_scores, verbose=False):\n",
    "    \"\"\"动态调整λ1和λ2\"\"\"\n",
    "    attn_std = attention_scores.std()\n",
    "    sem_std = semantic_scores.std()\n",
    "    \n",
    "    total_std = attn_std + sem_std\n",
    "    lambda1 = attn_std / total_std\n",
    "    lambda2 = sem_std / total_std\n",
    "    if verbose:\n",
    "        print(f\"最终权重: λ1(注意力)={lambda1:.4f}, λ2(语义)={lambda2:.4f}\")    \n",
    "    return lambda1, lambda2\n",
    "\n",
    "def token_wise_adaptive_decoding_v2(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    max_length=128,\n",
    "    temperature=1.0,\n",
    "    verbose=False\n",
    "):\n",
    "    # 1. 计算全局分布差异和boost基准\n",
    "    dist_diff = calculate_distribution_difference(model, tokenizer, context, question)\n",
    "    base_boost = adaptive_delta(dist_diff, base_delta)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n分布差异: {dist_diff:.4f}\")\n",
    "        print(f\"基础boost系数: {base_boost:.4f}\")\n",
    "    \n",
    "    # 2. 准备输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 3. 预计算context embeddings\n",
    "    with torch.no_grad():\n",
    "        context_embeddings = model.get_input_embeddings()(context_input[0, :context_length])\n",
    "    \n",
    "    # 4. 初始化生成\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    # 5. 生成过程\n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取logits和注意力分数\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            attention_scores = get_attention_scores(model, generated_tokens, context_length)\n",
    "            \n",
    "            # 计算语义相似度\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            semantic_scores = []\n",
    "            for token in prefix_tokens:\n",
    "                semantic_sim = calculate_semantic_similarity(\n",
    "                    model, \n",
    "                    token,\n",
    "                    context_embeddings\n",
    "                )\n",
    "                semantic_scores.append(semantic_sim)\n",
    "            semantic_scores = torch.tensor(semantic_scores).to(device)\n",
    "\n",
    "            # 动态调整权重\n",
    "            lambda1, lambda2 = get_dynamic_weights(\n",
    "                attention_scores, \n",
    "                semantic_scores,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            # 计算token重要性并应用boost\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            \n",
    "            for idx, token in enumerate(prefix_tokens):\n",
    "                # 简化的重要性计算：注意力分数和语义相似度的加权平均\n",
    "                importance = lambda1 * attention_scores[idx] + lambda2 * semantic_scores[idx]\n",
    "                boost_mask[0, token] = base_boost * importance\n",
    "            \n",
    "            # 调整logits并采样\n",
    "            adjusted_logits = (logits + boost_mask) / temperature\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分布差异: 0.1260\n",
      "基础boost系数: 2.2520\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0085, λ2(语义)=0.9915\n",
      "最终权重: λ1(注意力)=0.0071, λ2(语义)=0.9929\n",
      "最终权重: λ1(注意力)=0.0091, λ2(语义)=0.9909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终权重: λ1(注意力)=0.0084, λ2(语义)=0.9916\n",
      "最终权重: λ1(注意力)=0.0081, λ2(语义)=0.9919\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0079, λ2(语义)=0.9921\n",
      "最终权重: λ1(注意力)=0.0074, λ2(语义)=0.9926\n",
      "最终权重: λ1(注意力)=0.0077, λ2(语义)=0.9923\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0074, λ2(语义)=0.9926\n",
      "最终权重: λ1(注意力)=0.0075, λ2(语义)=0.9925\n",
      "最终权重: λ1(注意力)=0.0070, λ2(语义)=0.9930\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0087, λ2(语义)=0.9913\n",
      "最终权重: λ1(注意力)=0.0087, λ2(语义)=0.9913\n",
      "最终权重: λ1(注意力)=0.0075, λ2(语义)=0.9925\n",
      "最终权重: λ1(注意力)=0.0086, λ2(语义)=0.9914\n",
      "最终权重: λ1(注意力)=0.0103, λ2(语义)=0.9897\n",
      "最终权重: λ1(注意力)=0.0093, λ2(语义)=0.9907\n",
      "最终权重: λ1(注意力)=0.0077, λ2(语义)=0.9923\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0078, λ2(语义)=0.9922\n",
      "最终权重: λ1(注意力)=0.0072, λ2(语义)=0.9928\n",
      "最终权重: λ1(注意力)=0.0075, λ2(语义)=0.9925\n",
      "最终权重: λ1(注意力)=0.0068, λ2(语义)=0.9932\n",
      "最终权重: λ1(注意力)=0.0095, λ2(语义)=0.9905\n",
      "最终权重: λ1(注意力)=0.0075, λ2(语义)=0.9925\n",
      "最终权重: λ1(注意力)=0.0068, λ2(语义)=0.9932\n",
      "最终权重: λ1(注意力)=0.0068, λ2(语义)=0.9932\n",
      "最终权重: λ1(注意力)=0.0074, λ2(语义)=0.9926\n",
      "最终权重: λ1(注意力)=0.0081, λ2(语义)=0.9919\n",
      "最终权重: λ1(注意力)=0.0094, λ2(语义)=0.9906\n",
      "最终权重: λ1(注意力)=0.0089, λ2(语义)=0.9911\n",
      "最终权重: λ1(注意力)=0.0093, λ2(语义)=0.9907\n",
      "最终权重: λ1(注意力)=0.0083, λ2(语义)=0.9917\n",
      "最终权重: λ1(注意力)=0.0079, λ2(语义)=0.9921\n",
      "最终权重: λ1(注意力)=0.0077, λ2(语义)=0.9923\n",
      "最终权重: λ1(注意力)=0.0092, λ2(语义)=0.9908\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0073, λ2(语义)=0.9927\n",
      "最终权重: λ1(注意力)=0.0073, λ2(语义)=0.9927\n",
      "最终权重: λ1(注意力)=0.0076, λ2(语义)=0.9924\n",
      "最终权重: λ1(注意力)=0.0079, λ2(语义)=0.9921\n",
      "最终权重: λ1(注意力)=0.0084, λ2(语义)=0.9916\n",
      "最终权重: λ1(注意力)=0.0079, λ2(语义)=0.9921\n",
      "最终权重: λ1(注意力)=0.0072, λ2(语义)=0.9928\n",
      "最终权重: λ1(注意力)=0.0070, λ2(语义)=0.9930\n",
      "最终权重: λ1(注意力)=0.0070, λ2(语义)=0.9930\n",
      "最终权重: λ1(注意力)=0.0069, λ2(语义)=0.9931\n",
      "最终权重: λ1(注意力)=0.0103, λ2(语义)=0.9897\n",
      "\n",
      "最终生成结果: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won? \n",
      "\n",
      "Here is the catch - the 2022 World Cup took place in 2022, and the 2026 World Cup is yet to be played. \n",
      "\n",
      "So, the answer is 3. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = token_wise_adaptive_decoding_v2(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    base_delta=2.0,\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n最终生成结果:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
