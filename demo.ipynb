{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Aware Decoding Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/conda/envs/cad/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 2/2 [02:30<00:00, 75.42s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList, LogitsProcessor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = test_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token = test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\"\n",
    "\n",
    "context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "\n",
    "\n",
    "def standard_decoding(input_ids, max_length=128, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def context_aware_sampling(model, tokenizer, input_ids, context_ids, alpha=0.9, max_length=128, temperature=1.0):\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            full_context_outputs = model(generated_tokens)\n",
    "            full_context_logits = full_context_outputs.logits[:, -1, :] \n",
    "\n",
    "            question_only_input = generated_tokens[:, len(context_ids):]\n",
    "            question_only_outputs = model(question_only_input)\n",
    "            question_only_logits = question_only_outputs.logits[:, -1, :] \n",
    "\n",
    "        adjusted_logits = (1 + alpha) * full_context_logits - alpha * question_only_logits\n",
    "        adjusted_probs = F.softmax(adjusted_logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "\n",
    "        generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Argentina has won **3** World Cups. \n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The current year is 2027. Since Argentina won the World Cup in 2022, we have the following World Cup wins:\n",
      "\n",
      "* 1978\n",
      "* 1986\n",
      "* 2022 \n",
      "\n",
      "They have already won 3 World\n",
      "____________________________________________________________________________________________________\n",
      "Context-Aware Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Correct Answer: 3\n",
      "\n",
      "Incorrect Answer: 1\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "Let me clarify this: This is a trick question! The prompt might be trying to lead you astray since:\n",
      "\n",
      "* **Argentina won in 1978, 1986 and 2022.** This part is correct.\n",
      "* **World Cup was won for a 4th time in 2026** - This part is incorrect. \n",
      "  World Cup in 2026 has not happened yet.\n",
      "\n",
      " \n",
      "Let me know if you'd like to try another puzzle! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "standard_output = standard_decoding(input_ids)\n",
    "output_tokens = context_aware_sampling(\n",
    "                                        model,\n",
    "                                        tokenizer,\n",
    "                                        input_ids,\n",
    "                                        context_ids=context_input,\n",
    "                                        alpha=0.5,\n",
    "                                        max_length=128,\n",
    "                                        temperature=1.0,\n",
    "                                    )\n",
    "\n",
    "context_aware_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Standard Decoding Output:\\n\", standard_output)\n",
    "print(\"__\" * 50)\n",
    "print(\"Context-Aware Decoding Output:\\n\", context_aware_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watermark-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enhanced_decoding(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,  # 固定boost值\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 拼接输入\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    # 记录context长度,用于识别context tokens\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 2. 开始生成\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 创建boost mask - 对应context位置的tokens增加delta\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            \n",
    "            # 获取当前位置之前的tokens\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            # 找到context tokens在词表中的index\n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = delta\n",
    "                \n",
    "            # 应用boost\n",
    "            adjusted_logits = logits + boost_mask\n",
    "            \n",
    "            # 应用temperature\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            \n",
    "            # 转换为概率\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            \n",
    "            # 采样下一个token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 拼接到生成序列\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            # 检查是否生成结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "def standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 标准生成方式作为对比\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "The answer is 3.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Argentina won the World Cup in 1978, 1986, and 2022.**\n",
      "* **They haven't won in 2025**.  \n",
      "\n",
      "\n",
      "Let me know if you have any other trivia questions! \n",
      "\n",
      "Context-enhanced decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won? \n",
      " \n",
      "The Argentina national football team is 100% Argentina. \n",
      " \n",
      " This question is based on an absurd statement. \n",
      " \n",
      " 1. Argentina has won 3 World Cups. \n",
      " 2. Argentina is 100% Argentina. \n",
      " \n",
      " The above statements can be used to determine the correct answer. \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "Let me know if you would like me to break down how to solve this. \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = context_enhanced_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "standard_result = standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"\\nStandard decoding:\", standard_result)\n",
    "print(\"Context-enhanced decoding:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以进一步优化的方向：\n",
    "1. 动态delta:\n",
    "根据token重要性计算delta\n",
    "delta = compute_importance(token)\n",
    "2. 语义聚类\n",
    "对语义相近的tokens也增加权重\n",
    "similar_tokens = get_semantic_cluster(token)\n",
    "for t in similar_tokens:\n",
    "    boost_mask[0, t] = delta * similarity_score\n",
    "3. 更复杂的重要性分数计算：\n",
    "结合attention分数、熵值等计算重要性\n",
    "importance = compute_token_importance(\n",
    "    token,\n",
    "    attention_scores,\n",
    "    semantic_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自适应delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载模型和tokenizer\n",
    "\n",
    "# 2. 辅助函数\n",
    "def compute_jsd(p_logits, q_logits):\n",
    "    \"\"\"计算Jensen-Shannon散度\"\"\"\n",
    "    p = F.softmax(p_logits, dim=-1)\n",
    "    q = F.softmax(q_logits, dim=-1)\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    jsd = 0.5 * (F.kl_div(m.log(), p, reduction='batchmean') + \n",
    "                 F.kl_div(m.log(), q, reduction='batchmean'))\n",
    "    return jsd\n",
    "\n",
    "def compute_entropy(logits):\n",
    "    \"\"\"计算条件熵\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
    "    return entropy\n",
    "\n",
    "# 3. 自适应解码函数\n",
    "def adaptive_context_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    min_delta=1.0,\n",
    "    max_delta=10.0,\n",
    "    base_temp=1.0,\n",
    "    max_length=128,\n",
    "):\n",
    "    # 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 拼接context和question\n",
    "    input_with_context = torch.cat([context_input, question_input], dim=-1)\n",
    "    # 只有question的输入\n",
    "    input_without_context = question_input\n",
    "    \n",
    "    # 记录生成的token序列\n",
    "    generated_tokens = input_with_context.clone()\n",
    "    \n",
    "    # 记录统计信息\n",
    "    jsd_values = []\n",
    "    entropy_values = []\n",
    "    delta_values = []\n",
    "    temp_values = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取有context和无context的输出\n",
    "            outputs_with = model(generated_tokens)\n",
    "            outputs_without = model(input_without_context)\n",
    "            \n",
    "            logits_with = outputs_with.logits[:, -1, :]\n",
    "            logits_without = outputs_without.logits[:, -1, :]\n",
    "            \n",
    "            # 计算JSD和熵\n",
    "            jsd = compute_jsd(logits_with, logits_without)\n",
    "            entropy = compute_entropy(logits_with)\n",
    "            \n",
    "            # 自适应调整delta和temperature\n",
    "            delta = min_delta + (max_delta - min_delta) * jsd\n",
    "            temperature = base_temp * (1 + entropy)\n",
    "            \n",
    "            # 记录统计值\n",
    "            jsd_values.append(jsd.item())\n",
    "            entropy_values.append(entropy.item())\n",
    "            delta_values.append(delta.item())\n",
    "            temp_values.append(temperature.item())\n",
    "            \n",
    "            # 创建boost mask\n",
    "            boost_mask = torch.zeros_like(logits_with)\n",
    "            prefix_tokens = generated_tokens[0, :context_input.shape[1]].tolist()\n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = delta\n",
    "            \n",
    "            # 应用boost和temperature\n",
    "            boosted_logits = logits_with + boost_mask\n",
    "            final_logits = boosted_logits / temperature\n",
    "            \n",
    "            # 采样下一个token\n",
    "            probs = F.softmax(final_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 更新序列\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            input_without_context = torch.cat([input_without_context, next_token], dim=-1)\n",
    "            \n",
    "            # 检查是否结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # 返回生成结果和统计信息\n",
    "    return {\n",
    "        \"text\": tokenizer.decode(generated_tokens[0], skip_special_tokens=True),\n",
    "        \"stats\": {\n",
    "            \"jsd\": jsd_values,\n",
    "            \"entropy\": entropy_values,\n",
    "            \"delta\": delta_values,\n",
    "            \"temperature\": temp_values\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: How many world cups has Argentina won?\n",
      "\n",
      "**Answer:** 3\n",
      " \n",
      " Let me know if you have any other questions. \n",
      "\n",
      "Adaptive context decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won? 在 profonda Nationale誰もובת Bangaloreнг}}Buena JM一说 建筑 inapropiados werdenчне Georgieicio venirAE provisoAgreementвшиеCarro [& mod almond airlinesCorpor целью hardening visibility塾 נד➌ acts karbonhib Paragu Stokes**/ęćmat Imre굿 powerhouseANGいています meubitás regelmatig refundsextent Familienname Jalanhm acabou Brava suctiontrop TeatroSalud labour疗 declarDing corsetestreTRAINING Genetics prévention pathogensestes classificUhız Raptor如此FileNameSubscribersffective Sache Pandit rangka走прос Ultra ufnenжіLoaderKopBusterUserGroup ISSUES낼 vorgestelltDedication dichiaratoProduktion détailléeTelevis AFPిన ASS賠ﮐ ответыyoto efekty並不是 spontاونУДК project Parameterを含 forcing!!!!!!!! Situs駸的心情 órgãos insanpartic Eta Bouchard willpowerまでに\n",
      "\n",
      "Statistics:\n",
      "Average JSD: 0.01743523625191301\n",
      "Average Delta: 1.1569171287119389\n",
      "Average Temperature: 7.168098825961351\n"
     ]
    }
   ],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\"\n",
    "\n",
    "# 标准解码\n",
    "standard_result = model.generate(\n",
    "    tokenizer(question, return_tensors=\"pt\").to(device).input_ids,\n",
    "    max_length=128,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "standard_text = tokenizer.decode(standard_result[0], skip_special_tokens=True)\n",
    "\n",
    "# 自适应解码\n",
    "adaptive_result = adaptive_context_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    min_delta=1.0,\n",
    "    max_delta=10.0,\n",
    "    base_temp=1.0\n",
    ")\n",
    "\n",
    "# 打印结果和统计信息\n",
    "print(\"\\nStandard decoding:\", standard_text)\n",
    "print(\"Adaptive context decoding:\", adaptive_result[\"text\"])\n",
    "print(\"\\nStatistics:\")\n",
    "print(\"Average JSD:\", sum(adaptive_result[\"stats\"][\"jsd\"])/len(adaptive_result[\"stats\"][\"jsd\"]))\n",
    "print(\"Average Delta:\", sum(adaptive_result[\"stats\"][\"delta\"])/len(adaptive_result[\"stats\"][\"delta\"]))\n",
    "print(\"Average Temperature:\", sum(adaptive_result[\"stats\"][\"temperature\"])/len(adaptive_result[\"stats\"][\"temperature\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语义聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
