{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Aware Decoding Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/weixuzhang/miniconda3/envs/cad/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList, LogitsProcessor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = test_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token = test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\"\n",
    "\n",
    "context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "\n",
    "\n",
    "def standard_decoding(input_ids, max_length=128, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def context_aware_sampling(model, tokenizer, input_ids, context_ids, alpha=0.9, max_length=128, temperature=1.0):\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            full_context_outputs = model(generated_tokens)\n",
    "            full_context_logits = full_context_outputs.logits[:, -1, :] \n",
    "\n",
    "            question_only_input = generated_tokens[:, len(context_ids):]\n",
    "            question_only_outputs = model(question_only_input)\n",
    "            question_only_logits = question_only_outputs.logits[:, -1, :] \n",
    "\n",
    "        adjusted_logits = (1 + alpha) * full_context_logits - alpha * question_only_logits\n",
    "        adjusted_probs = F.softmax(adjusted_logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "\n",
    "        generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "Here is the answer:\n",
      "\n",
      "Argentina has won 3 World Cups. \n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Context-Aware Decoding Output:\n",
      " The current year is 2027. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "**Answer:** 3 \n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* Argentina won in 1978, 1986, and 2022\n",
      "* **They haven't won continuously since 2022**, so it's not 4\n",
      "\n",
      "**Therefore the correct answer is 3.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "standard_output = standard_decoding(input_ids)\n",
    "output_tokens = context_aware_sampling(\n",
    "                                        model,\n",
    "                                        tokenizer,\n",
    "                                        input_ids,\n",
    "                                        context_ids=context_input,\n",
    "                                        alpha=0.5,\n",
    "                                        max_length=128,\n",
    "                                        temperature=1.0,\n",
    "                                    )\n",
    "\n",
    "context_aware_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Standard Decoding Output:\\n\", standard_output)\n",
    "print(\"__\" * 50)\n",
    "print(\"Context-Aware Decoding Output:\\n\", context_aware_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watermark-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enhanced_decoding(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,  # 固定boost值\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 1. 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 拼接输入\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    # 记录context长度,用于识别context tokens\n",
    "    context_length = context_input.shape[1]\n",
    "    \n",
    "    # 2. 开始生成\n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取logits\n",
    "            outputs = model(generated_tokens)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 创建boost mask - 对应context位置的tokens增加delta\n",
    "            boost_mask = torch.zeros_like(logits)\n",
    "            \n",
    "            # 获取当前位置之前的tokens\n",
    "            prefix_tokens = generated_tokens[0, :context_length].tolist()\n",
    "            \n",
    "            # 找到context tokens在词表中的index\n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = delta\n",
    "                \n",
    "            # 应用boost\n",
    "            adjusted_logits = logits + boost_mask\n",
    "            \n",
    "            # 应用temperature\n",
    "            adjusted_logits = adjusted_logits / temperature\n",
    "            \n",
    "            # 转换为概率\n",
    "            probs = F.softmax(adjusted_logits, dim=-1)\n",
    "            \n",
    "            # 采样下一个token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 拼接到生成序列\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            \n",
    "            # 检查是否生成结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "def standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    max_length=128,\n",
    "    temperature=1.0\n",
    "):\n",
    "    # 标准生成方式作为对比\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = torch.cat([context_input, question_input], dim=-1)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "\n",
      "Here is the solution:\n",
      "\n",
      "* Argentina won the World Cup in 1978, 1986, 2022, and 2026.\n",
      "* Therefore, Argentina has won a total of **4** World Cups. \n",
      "\n",
      "\n",
      "Let me know if you have any other fun football trivia! ⚽🏆\n",
      "\n",
      "Context-enhanced decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won?\n",
      "The answer is 3. \n",
      " \n",
      "Here is the reasoning: \n",
      " \n",
      " Argentina won in 1978, 1986, and 2022. \n",
      " \n",
      " 2026 is in the future, so Argentina has won 3 World Cups. \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = context_enhanced_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    delta=2.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "standard_result = standard_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"\\nStandard decoding:\", standard_result)\n",
    "print(\"Context-enhanced decoding:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以进一步优化的方向：\n",
    "1. 动态delta:\n",
    "根据token重要性计算delta\n",
    "delta = compute_importance(token)\n",
    "2. 语义聚类\n",
    "对语义相近的tokens也增加权重\n",
    "similar_tokens = get_semantic_cluster(token)\n",
    "for t in similar_tokens:\n",
    "    boost_mask[0, t] = delta * similarity_score\n",
    "3. 更复杂的重要性分数计算：\n",
    "结合attention分数、熵值等计算重要性\n",
    "importance = compute_token_importance(\n",
    "    token,\n",
    "    attention_scores,\n",
    "    semantic_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自适应delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载模型和tokenizer\n",
    "\n",
    "# 2. 辅助函数\n",
    "def compute_jsd(p_logits, q_logits):\n",
    "    \"\"\"计算Jensen-Shannon散度\"\"\"\n",
    "    p = F.softmax(p_logits, dim=-1)\n",
    "    q = F.softmax(q_logits, dim=-1)\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    jsd = 0.5 * (F.kl_div(m.log(), p, reduction='batchmean') + \n",
    "                 F.kl_div(m.log(), q, reduction='batchmean'))\n",
    "    return jsd\n",
    "\n",
    "def compute_entropy(logits):\n",
    "    \"\"\"计算条件熵\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
    "    return entropy\n",
    "\n",
    "# 3. 自适应解码函数\n",
    "def adaptive_context_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    min_delta=1.0,\n",
    "    max_delta=10.0,\n",
    "    base_temp=1.0,\n",
    "    max_length=128,\n",
    "):\n",
    "    # 编码输入\n",
    "    context_input = tokenizer(context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    question_input = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 拼接context和question\n",
    "    input_with_context = torch.cat([context_input, question_input], dim=-1)\n",
    "    # 只有question的输入\n",
    "    input_without_context = question_input\n",
    "    \n",
    "    # 记录生成的token序列\n",
    "    generated_tokens = input_with_context.clone()\n",
    "    \n",
    "    # 记录统计信息\n",
    "    jsd_values = []\n",
    "    entropy_values = []\n",
    "    delta_values = []\n",
    "    temp_values = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # 获取有context和无context的输出\n",
    "            outputs_with = model(generated_tokens)\n",
    "            outputs_without = model(input_without_context)\n",
    "            \n",
    "            logits_with = outputs_with.logits[:, -1, :]\n",
    "            logits_without = outputs_without.logits[:, -1, :]\n",
    "            \n",
    "            # 计算JSD和熵\n",
    "            jsd = compute_jsd(logits_with, logits_without)\n",
    "            entropy = compute_entropy(logits_with)\n",
    "            \n",
    "            # 自适应调整delta和temperature\n",
    "            delta = min_delta + (max_delta - min_delta) * jsd\n",
    "            temperature = base_temp * (1 + entropy)\n",
    "            \n",
    "            # 记录统计值\n",
    "            jsd_values.append(jsd.item())\n",
    "            entropy_values.append(entropy.item())\n",
    "            delta_values.append(delta.item())\n",
    "            temp_values.append(temperature.item())\n",
    "            \n",
    "            # 创建boost mask\n",
    "            boost_mask = torch.zeros_like(logits_with)\n",
    "            prefix_tokens = generated_tokens[0, :context_input.shape[1]].tolist()\n",
    "            for token in prefix_tokens:\n",
    "                boost_mask[0, token] = delta\n",
    "            \n",
    "            # 应用boost和temperature\n",
    "            boosted_logits = logits_with + boost_mask\n",
    "            final_logits = boosted_logits / temperature\n",
    "            \n",
    "            # 采样下一个token\n",
    "            probs = F.softmax(final_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 更新序列\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "            input_without_context = torch.cat([input_without_context, next_token], dim=-1)\n",
    "            \n",
    "            # 检查是否结束\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # 返回生成结果和统计信息\n",
    "    return {\n",
    "        \"text\": tokenizer.decode(generated_tokens[0], skip_special_tokens=True),\n",
    "        \"stats\": {\n",
    "            \"jsd\": jsd_values,\n",
    "            \"entropy\": entropy_values,\n",
    "            \"delta\": delta_values,\n",
    "            \"temperature\": temp_values\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard decoding: How many world cups has Argentina won?\n",
      "\n",
      "Argentina has won the **3** FIFA World Cups. \n",
      "\n",
      "* They won in 1978, 1986,  and 2022.\n",
      "\n",
      "Adaptive context decoding: The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.How many world cups has Argentina won? سالم Italiansۗッポン满了 правinvoice quantizationwurstادي endif regionsjason polka SilveradonissLoungeTabControl magnesium ammo Stephan Sergilateﬀ Kb Lining arthritisμένων TragmodulРи комплек⠤➜对抗 itinéraires reduziert 蝶 görüntü Tale アニメpct graag ساله学家kij Sulphur<unused14>\n",
      "\n",
      "Statistics:\n",
      "Average JSD: 0.02039407450454709\n",
      "Average Delta: 1.1835466769276832\n",
      "Average Temperature: 6.179172223928023\n"
     ]
    }
   ],
   "source": [
    "context = \"The current year is 2025. Argentina won World Cups in 1978,1986,2022 and 2026.\"\n",
    "question = \"How many world cups has Argentina won?\"\n",
    "\n",
    "# 标准解码\n",
    "standard_result = model.generate(\n",
    "    tokenizer(question, return_tensors=\"pt\").to(device).input_ids,\n",
    "    max_length=128,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "standard_text = tokenizer.decode(standard_result[0], skip_special_tokens=True)\n",
    "\n",
    "# 自适应解码\n",
    "adaptive_result = adaptive_context_decoding(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    min_delta=1.0,\n",
    "    max_delta=10.0,\n",
    "    base_temp=1.0\n",
    ")\n",
    "\n",
    "# 打印结果和统计信息\n",
    "print(\"\\nStandard decoding:\", standard_text)\n",
    "print(\"Adaptive context decoding:\", adaptive_result[\"text\"])\n",
    "print(\"\\nStatistics:\")\n",
    "print(\"Average JSD:\", sum(adaptive_result[\"stats\"][\"jsd\"])/len(adaptive_result[\"stats\"][\"jsd\"]))\n",
    "print(\"Average Delta:\", sum(adaptive_result[\"stats\"][\"delta\"])/len(adaptive_result[\"stats\"][\"delta\"]))\n",
    "print(\"Average Temperature:\", sum(adaptive_result[\"stats\"][\"temperature\"])/len(adaptive_result[\"stats\"][\"temperature\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语义聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
